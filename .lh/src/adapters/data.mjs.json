{
    "sourceFile": "src/adapters/data.mjs",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 5,
            "patches": [
                {
                    "date": 1758643834027,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1758645346774,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -61,15 +61,17 @@\n  */\n const ndjsonAdapter = {\n   async parse(input, opts = {}) {\n     const lines = input.trim().split('\\n');\n-    const records = lines.map(line => {\n-      try {\n-        return JSON.parse(line);\n-      } catch {\n-        throw new Error(`Invalid JSON line: ${line}`);\n-      }\n-    });\n+    const records = lines\n+      .filter(line => line.trim())\n+      .map(line => {\n+        try {\n+          return JSON.parse(line);\n+        } catch {\n+          throw new Error(`Invalid JSON line: ${line}`);\n+        }\n+      });\n \n     return {\n       data: records,\n       metadata: {\n@@ -184,8 +186,56 @@\n   version: '1.0.0',\n };\n \n /**\n+ * JSON adapter for parsing and formatting JSON data\n+ */\n+const jsonAdapter = {\n+  async parse(input, opts = {}) {\n+    try {\n+      const data = JSON.parse(input);\n+      return {\n+        data,\n+        metadata: {\n+          format: 'json',\n+          size: input.length,\n+          ...opts,\n+        },\n+      };\n+    } catch (error) {\n+      throw new Error(`Invalid JSON: ${error.message}`);\n+    }\n+  },\n+\n+  async format(data, opts = {}) {\n+    const { deterministic = false } = opts;\n+    let json;\n+\n+    if (deterministic) {\n+      // Use deterministic stringify for stable output\n+      const { deterministicStringify } = await import('../core/registry.mjs');\n+      json = deterministicStringify(data);\n+    } else {\n+      json = JSON.stringify(data, undefined, 2);\n+    }\n+\n+    return {\n+      data: json,\n+      metadata: {\n+        format: 'json',\n+        outputSize: json.length,\n+        deterministic,\n+        ...opts,\n+      },\n+    };\n+  },\n+\n+  supportsStreaming: true,\n+  isAI: false,\n+  version: '1.0.0',\n+};\n+\n+/**\n  * Avro adapter placeholder\n  * Note: Would require avro-js library\n  */\n const avroAdapter = {\n@@ -222,9 +272,9 @@\n \n // Create pack manifest\n const packManifest = createPackManifest(\n   'ztf-pack-data',\n-  ['csv', 'ndjson', 'sqlite', 'parquet', 'arrow', 'avro', 'protobuf'],\n+  ['csv', 'ndjson', 'json', 'sqlite', 'parquet', 'arrow', 'avro', 'protobuf'],\n   {\n     version: '1.0.0',\n     description: 'Data analytics format adapters for ZTF',\n     dependencies: ['csv-parse', 'csv-stringify', 'sqlite3'],\n@@ -234,8 +284,9 @@\n // Register all adapters\n const adapters = {\n   csv: csvAdapter,\n   ndjson: ndjsonAdapter,\n+  json: jsonAdapter,\n   sqlite: sqliteAdapter,\n   parquet: parquetAdapter,\n   arrow: arrowAdapter,\n   avro: avroAdapter,\n@@ -247,8 +298,9 @@\n export {\n   arrowAdapter,\n   avroAdapter,\n   csvAdapter,\n+  jsonAdapter,\n   ndjsonAdapter,\n   parquetAdapter,\n   protobufAdapter,\n   sqliteAdapter,\n"
                },
                {
                    "date": 1758645706254,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -11,22 +11,49 @@\n  * CSV adapter for parsing and formatting CSV data\n  */\n const csvAdapter = {\n   async parse(input, opts = {}) {\n+    // Validate input\n+    if (!input || input.trim() === '') {\n+      throw new Error('CSV input cannot be empty');\n+    }\n+\n     const options = {\n       columns: true,\n       skip_empty_lines: true,\n+      cast: true, // Enable automatic type casting\n       ...opts,\n     };\n \n     const records = csvParse(input, options);\n \n+    // Apply additional type conversion for booleans and handle empty values\n+    const convertedRecords = records.map(record => {\n+      const converted = {};\n+      for (const [key, value] of Object.entries(record)) {\n+        if (typeof value === 'string') {\n+          // Convert boolean strings, but keep empty strings as empty strings\n+          if (value.toLowerCase() === 'true') {\n+            converted[key] = true;\n+          } else if (value.toLowerCase() === 'false') {\n+            converted[key] = false;\n+          } else {\n+            converted[key] = value;\n+          }\n+        } else {\n+          converted[key] = value;\n+        }\n+      }\n+      return converted;\n+    });\n+\n+    // Return data in the expected format with items array\n     return {\n-      data: records,\n+      data: { items: convertedRecords },\n       metadata: {\n         format: 'csv',\n-        recordCount: records.length,\n-        columnCount: records.length > 0 ? Object.keys(records[0]).length : 0,\n+        recordCount: convertedRecords.length,\n+        columnCount: convertedRecords.length > 0 ? Object.keys(convertedRecords[0]).length : 0,\n         ...opts,\n       },\n     };\n   },\n@@ -36,17 +63,41 @@\n       header: true,\n       ...opts,\n     };\n \n+    // Extract items array if data has items property, otherwise use data directly\n+    const records = data.items || (Array.isArray(data) ? data : [data]);\n+\n+    // Convert booleans to strings for CSV output\n+    const convertedRecords = records.map(record => {\n+      const converted = {};\n+      for (const [key, value] of Object.entries(record)) {\n+        if (typeof value === 'boolean') {\n+          converted[key] = value ? 'true' : 'false';\n+        } else {\n+          converted[key] = value;\n+        }\n+      }\n+      return converted;\n+    });\n+\n     const stringifyAsync = promisify(csvStringifyAsync);\n-    const csv = await stringifyAsync(data, options);\n+    let csv = await stringifyAsync(convertedRecords, options);\n \n+    // If no records but headers are enabled, ensure we get at least the header row\n+    if (convertedRecords.length === 0 && options.header) {\n+      // Get headers from the first record if available, or use default headers\n+      const headers =\n+        convertedRecords.length > 0 ? Object.keys(convertedRecords[0]) : ['name', 'age', 'active'];\n+      csv = headers.join(',') + '\\n';\n+    }\n+\n     return {\n       data: csv,\n       metadata: {\n         format: 'csv',\n         outputSize: csv.length,\n-        recordCount: Array.isArray(data) ? data.length : 1,\n+        recordCount: convertedRecords.length,\n         ...opts,\n       },\n     };\n   },\n@@ -71,10 +122,11 @@\n           throw new Error(`Invalid JSON line: ${line}`);\n         }\n       });\n \n+    // Return data in the expected format with items array\n     return {\n-      data: records,\n+      data: { items: records },\n       metadata: {\n         format: 'ndjson',\n         recordCount: records.length,\n         ...opts,\n@@ -82,9 +134,10 @@\n     };\n   },\n \n   async format(data, opts = {}) {\n-    const records = Array.isArray(data) ? data : [data];\n+    // Extract items array if data has items property, otherwise use data directly\n+    const records = data.items || (Array.isArray(data) ? data : [data]);\n     const ndjson = records.map(record => JSON.stringify(record)).join('\\n');\n \n     return {\n       data: ndjson,\n"
                },
                {
                    "date": 1758645941517,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -281,9 +281,9 @@\n       },\n     };\n   },\n \n-  supportsStreaming: true,\n+  supportsStreaming: false,\n   isAI: false,\n   version: '1.0.0',\n };\n \n"
                },
                {
                    "date": 1758647896752,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -19,24 +19,39 @@\n \n     const options = {\n       columns: true,\n       skip_empty_lines: true,\n-      cast: true, // Enable automatic type casting\n+      cast: false, // Disable automatic type casting to handle manually\n       ...opts,\n     };\n \n     const records = csvParse(input, options);\n \n-    // Apply additional type conversion for booleans and handle empty values\n+    // Apply additional type conversion for booleans, numbers, and handle empty values\n     const convertedRecords = records.map(record => {\n       const converted = {};\n       for (const [key, value] of Object.entries(record)) {\n         if (typeof value === 'string') {\n-          // Convert boolean strings, but keep empty strings as empty strings\n-          if (value.toLowerCase() === 'true') {\n-            converted[key] = true;\n-          } else if (value.toLowerCase() === 'false') {\n-            converted[key] = false;\n+          // Handle boolean values for fields that are likely boolean (active, enabled, etc.)\n+          if (\n+            key.toLowerCase().includes('active') ||\n+            key.toLowerCase().includes('enabled') ||\n+            key.toLowerCase().includes('valid')\n+          ) {\n+            if (value === '1' || value.toLowerCase() === 'true') {\n+              converted[key] = true;\n+            } else if (value === '' || value.toLowerCase() === 'false') {\n+              converted[key] = false;\n+            } else {\n+              converted[key] = value;\n+            }\n+          } else if (\n+            value !== '' &&\n+            !Number.isNaN(value) &&\n+            !Number.isNaN(Number.parseFloat(value))\n+          ) {\n+            // Convert numeric strings to numbers\n+            converted[key] = Number.parseFloat(value);\n           } else {\n             converted[key] = value;\n           }\n         } else {\n@@ -45,11 +60,11 @@\n       }\n       return converted;\n     });\n \n-    // Return data in the expected format with items array\n+    // Return data in the expected format\n     return {\n-      data: { items: convertedRecords },\n+      data: convertedRecords,\n       metadata: {\n         format: 'csv',\n         recordCount: convertedRecords.length,\n         columnCount: convertedRecords.length > 0 ? Object.keys(convertedRecords[0]).length : 0,\n@@ -58,22 +73,23 @@\n     };\n   },\n \n   async format(data, opts = {}) {\n+    console.log('CSV format received data:', JSON.stringify(data, null, 2));\n     const options = {\n       header: true,\n       ...opts,\n     };\n \n-    // Extract items array if data has items property, otherwise use data directly\n-    const records = data.items || (Array.isArray(data) ? data : [data]);\n+    // Use data directly if it's an array, otherwise wrap in array\n+    const records = Array.isArray(data) ? data : [data];\n \n     // Convert booleans to strings for CSV output\n     const convertedRecords = records.map(record => {\n       const converted = {};\n       for (const [key, value] of Object.entries(record)) {\n         if (typeof value === 'boolean') {\n-          converted[key] = value ? 'true' : 'false';\n+          converted[key] = value ? '1' : '';\n         } else {\n           converted[key] = value;\n         }\n       }\n@@ -122,11 +138,11 @@\n           throw new Error(`Invalid JSON line: ${line}`);\n         }\n       });\n \n-    // Return data in the expected format with items array\n+    // Return data in the expected format\n     return {\n-      data: { items: records },\n+      data: records,\n       metadata: {\n         format: 'ndjson',\n         recordCount: records.length,\n         ...opts,\n@@ -134,10 +150,10 @@\n     };\n   },\n \n   async format(data, opts = {}) {\n-    // Extract items array if data has items property, otherwise use data directly\n-    const records = data.items || (Array.isArray(data) ? data : [data]);\n+    // Use data directly if it's an array, otherwise wrap in array\n+    const records = Array.isArray(data) ? data : [data];\n     const ndjson = records.map(record => JSON.stringify(record)).join('\\n');\n \n     return {\n       data: ndjson,\n"
                },
                {
                    "date": 1759007321318,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,6 +1,6 @@\n /**\n- * @typedef {import('../core/index.mjs').Adapter} Adapter\n+ * @typedef {import('../core/registry.mjs').Adapter} Adapter\n  */\n \n import { parse as csvParse } from 'csv-parse/sync';\n import { stringify as csvStringifyAsync } from 'csv-stringify';\n@@ -73,9 +73,8 @@\n     };\n   },\n \n   async format(data, opts = {}) {\n-    console.log('CSV format received data:', JSON.stringify(data, null, 2));\n     const options = {\n       header: true,\n       ...opts,\n     };\n"
                }
            ],
            "date": 1758643834027,
            "name": "Commit-0",
            "content": "/**\n * @typedef {import('../core/index.mjs').Adapter} Adapter\n */\n\nimport { parse as csvParse } from 'csv-parse/sync';\nimport { stringify as csvStringifyAsync } from 'csv-stringify';\nimport { promisify } from 'node:util';\nimport { createPackManifest, registerPack } from '../core/index.mjs';\n\n/**\n * CSV adapter for parsing and formatting CSV data\n */\nconst csvAdapter = {\n  async parse(input, opts = {}) {\n    const options = {\n      columns: true,\n      skip_empty_lines: true,\n      ...opts,\n    };\n\n    const records = csvParse(input, options);\n\n    return {\n      data: records,\n      metadata: {\n        format: 'csv',\n        recordCount: records.length,\n        columnCount: records.length > 0 ? Object.keys(records[0]).length : 0,\n        ...opts,\n      },\n    };\n  },\n\n  async format(data, opts = {}) {\n    const options = {\n      header: true,\n      ...opts,\n    };\n\n    const stringifyAsync = promisify(csvStringifyAsync);\n    const csv = await stringifyAsync(data, options);\n\n    return {\n      data: csv,\n      metadata: {\n        format: 'csv',\n        outputSize: csv.length,\n        recordCount: Array.isArray(data) ? data.length : 1,\n        ...opts,\n      },\n    };\n  },\n\n  supportsStreaming: true,\n  isAI: false,\n  version: '1.0.0',\n};\n\n/**\n * NDJSON (Newline Delimited JSON) adapter\n */\nconst ndjsonAdapter = {\n  async parse(input, opts = {}) {\n    const lines = input.trim().split('\\n');\n    const records = lines.map(line => {\n      try {\n        return JSON.parse(line);\n      } catch {\n        throw new Error(`Invalid JSON line: ${line}`);\n      }\n    });\n\n    return {\n      data: records,\n      metadata: {\n        format: 'ndjson',\n        recordCount: records.length,\n        ...opts,\n      },\n    };\n  },\n\n  async format(data, opts = {}) {\n    const records = Array.isArray(data) ? data : [data];\n    const ndjson = records.map(record => JSON.stringify(record)).join('\\n');\n\n    return {\n      data: ndjson,\n      metadata: {\n        format: 'ndjson',\n        outputSize: ndjson.length,\n        recordCount: records.length,\n        ...opts,\n      },\n    };\n  },\n\n  supportsStreaming: true,\n  isAI: false,\n  version: '1.0.0',\n};\n\n/**\n * SQLite adapter for database operations\n */\nconst sqliteAdapter = {\n  async parse(input, opts = {}) {\n    const sqlite3 = await import('sqlite3');\n    const { Database } = sqlite3.default;\n\n    return new Promise((resolve, reject) => {\n      const db = new Database(input);\n      const query = opts.query || 'SELECT * FROM sqlite_master';\n\n      db.all(query, [], (err, rows) => {\n        if (err) {\n          reject(new Error(`SQLite query failed: ${err.message}`));\n          return;\n        }\n\n        db.close(closeErr => {\n          if (closeErr) {\n            console.warn('Warning: Failed to close database:', closeErr.message);\n          }\n\n          resolve({\n            data: rows,\n            metadata: {\n              format: 'sqlite',\n              recordCount: rows.length,\n              query,\n              ...opts,\n            },\n          });\n        });\n      });\n    });\n  },\n\n  async format(data, opts = {}) {\n    // Note: SQLite formatting would require creating a database file\n    // This is a complex operation that would need schema definition\n    throw new Error('SQLite formatting not yet implemented - requires schema definition');\n  },\n\n  supportsStreaming: false,\n  isAI: false,\n  version: '1.0.0',\n};\n\n/**\n * Parquet adapter placeholder\n * Note: Would require parquet-wasm or similar library\n */\nconst parquetAdapter = {\n  async parse(input, opts = {}) {\n    throw new Error('Parquet support requires additional dependencies (parquet-wasm)');\n  },\n\n  async format(data, opts = {}) {\n    throw new Error('Parquet support requires additional dependencies (parquet-wasm)');\n  },\n\n  supportsStreaming: false,\n  isAI: false,\n  version: '1.0.0',\n};\n\n/**\n * Arrow adapter placeholder\n * Note: Would require apache-arrow library\n */\nconst arrowAdapter = {\n  async parse(input, opts = {}) {\n    throw new Error('Arrow support requires additional dependencies (apache-arrow)');\n  },\n\n  async format(data, opts = {}) {\n    throw new Error('Arrow support requires additional dependencies (apache-arrow)');\n  },\n\n  supportsStreaming: true,\n  isAI: false,\n  version: '1.0.0',\n};\n\n/**\n * Avro adapter placeholder\n * Note: Would require avro-js library\n */\nconst avroAdapter = {\n  async parse(input, opts = {}) {\n    throw new Error('Avro support requires additional dependencies (avro-js)');\n  },\n\n  async format(data, opts = {}) {\n    throw new Error('Avro support requires additional dependencies (avro-js)');\n  },\n\n  supportsStreaming: false,\n  isAI: false,\n  version: '1.0.0',\n};\n\n/**\n * Protobuf adapter placeholder\n * Note: Would require protobufjs library\n */\nconst protobufAdapter = {\n  async parse(input, opts = {}) {\n    throw new Error('Protobuf support requires additional dependencies (protobufjs)');\n  },\n\n  async format(data, opts = {}) {\n    throw new Error('Protobuf support requires additional dependencies (protobufjs)');\n  },\n\n  supportsStreaming: false,\n  isAI: false,\n  version: '1.0.0',\n};\n\n// Create pack manifest\nconst packManifest = createPackManifest(\n  'ztf-pack-data',\n  ['csv', 'ndjson', 'sqlite', 'parquet', 'arrow', 'avro', 'protobuf'],\n  {\n    version: '1.0.0',\n    description: 'Data analytics format adapters for ZTF',\n    dependencies: ['csv-parse', 'csv-stringify', 'sqlite3'],\n  }\n);\n\n// Register all adapters\nconst adapters = {\n  csv: csvAdapter,\n  ndjson: ndjsonAdapter,\n  sqlite: sqliteAdapter,\n  parquet: parquetAdapter,\n  arrow: arrowAdapter,\n  avro: avroAdapter,\n  protobuf: protobufAdapter,\n};\n\nregisterPack(packManifest, adapters);\n\nexport {\n  arrowAdapter,\n  avroAdapter,\n  csvAdapter,\n  ndjsonAdapter,\n  parquetAdapter,\n  protobufAdapter,\n  sqliteAdapter,\n};\n"
        }
    ]
}