{
    "sourceFile": "test/features/integration.test.mjs",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 1,
            "patches": [
                {
                    "date": 1758645130124,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1758645941438,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -4,10 +4,9 @@\n  */\n \n import { beforeEach, describe, expect, it } from 'vitest';\n import { z } from 'zod';\n-import { convert, formatTo, parseFrom } from '../../src/core/main.mjs';\n-import { registerAdapter } from '../../src/core/registry.mjs';\n+import { convert, formatTo, parseFrom, registerAdapter } from '../setup.mjs';\n \n describe('Integration Tests', () => {\n   const UserSchema = z.object({\n     id: z.string(),\n"
                }
            ],
            "date": 1758645130124,
            "name": "Commit-0",
            "content": "/**\n * Integration Tests\n * @fileoverview Complex workflow and adapter interaction tests\n */\n\nimport { beforeEach, describe, expect, it } from 'vitest';\nimport { z } from 'zod';\nimport { convert, formatTo, parseFrom } from '../../src/core/main.mjs';\nimport { registerAdapter } from '../../src/core/registry.mjs';\n\ndescribe('Integration Tests', () => {\n  const UserSchema = z.object({\n    id: z.string(),\n    name: z.string(),\n    email: z.string().email(),\n    age: z.number().min(0).max(150),\n    active: z.boolean(),\n    metadata: z.object({\n      tags: z.array(z.string()),\n      score: z.number().min(0).max(100),\n      lastLogin: z.string().optional(),\n    }),\n  });\n\n  const UserArraySchema = z.object({\n    users: z.array(UserSchema),\n    total: z.number(),\n    page: z.number(),\n    limit: z.number(),\n  });\n\n  beforeEach(() => {\n    // Ensure clean state for each test\n  });\n\n  describe('Complex Data Workflows', () => {\n    it('should handle complex user data through multiple format conversions', async () => {\n      const userData = {\n        users: [\n          {\n            id: 'user-1',\n            name: 'John Doe',\n            email: 'john@example.com',\n            age: 30,\n            active: true,\n            metadata: {\n              tags: ['premium', 'verified'],\n              score: 85,\n              lastLogin: '2024-01-15T10:30:00Z',\n            },\n          },\n          {\n            id: 'user-2',\n            name: 'Jane Smith',\n            email: 'jane@example.com',\n            age: 25,\n            active: false,\n            metadata: {\n              tags: ['basic'],\n              score: 60,\n            },\n          },\n        ],\n        total: 2,\n        page: 1,\n        limit: 10,\n      };\n\n      // Convert through multiple formats\n      const jsonResult = await formatTo(UserArraySchema, 'json', userData, { deterministic: true });\n      const csvResult = await convert(UserArraySchema, { from: 'json', to: 'csv' }, jsonResult);\n      const ndjsonResult = await convert(UserArraySchema, { from: 'csv', to: 'ndjson' }, csvResult);\n      const finalJsonResult = await convert(\n        UserArraySchema,\n        { from: 'ndjson', to: 'json' },\n        ndjsonResult\n      );\n\n      // Parse final result and verify data integrity\n      const finalParsed = JSON.parse(finalJsonResult);\n      expect(finalParsed.users).toHaveLength(2);\n      expect(finalParsed.total).toBe(2);\n      expect(finalParsed.page).toBe(1);\n      expect(finalParsed.limit).toBe(10);\n    });\n\n    it('should handle large dataset processing workflow', async () => {\n      // Generate large dataset\n      const largeDataset = {\n        users: Array.from({ length: 1000 }, (_, i) => ({\n          id: `user-${i + 1}`,\n          name: `User ${i + 1}`,\n          email: `user${i + 1}@example.com`,\n          age: 20 + (i % 50),\n          active: i % 2 === 0,\n          metadata: {\n            tags: [`tag-${i % 10}`],\n            score: 50 + (i % 50),\n          },\n        })),\n        total: 1000,\n        page: 1,\n        limit: 1000,\n      };\n\n      const startTime = Date.now();\n\n      // Process through multiple formats\n      const jsonResult = await formatTo(UserArraySchema, 'json', largeDataset, {\n        deterministic: true,\n      });\n      const csvResult = await convert(UserArraySchema, { from: 'json', to: 'csv' }, jsonResult);\n      const ndjsonResult = await convert(UserArraySchema, { from: 'csv', to: 'ndjson' }, csvResult);\n      const finalJsonResult = await convert(\n        UserArraySchema,\n        { from: 'ndjson', to: 'json' },\n        ndjsonResult\n      );\n\n      const endTime = Date.now();\n      const duration = endTime - startTime;\n\n      // Verify data integrity\n      const finalParsed = JSON.parse(finalJsonResult);\n      expect(finalParsed.users).toHaveLength(1000);\n      expect(finalParsed.total).toBe(1000);\n\n      // Verify performance\n      expect(duration).toBeLessThan(10_000); // Should complete within 10 seconds\n    });\n\n    it('should handle streaming workflow with large data', async () => {\n      const largeDataset = {\n        users: Array.from({ length: 5000 }, (_, i) => ({\n          id: `user-${i + 1}`,\n          name: `User ${i + 1}`,\n          email: `user${i + 1}@example.com`,\n          age: 20 + (i % 50),\n          active: i % 2 === 0,\n          metadata: {\n            tags: [`tag-${i % 10}`],\n            score: 50 + (i % 50),\n          },\n        })),\n        total: 5000,\n        page: 1,\n        limit: 5000,\n      };\n\n      const startTime = Date.now();\n\n      // Use streaming for large data\n      const csvResult = await formatTo(UserArraySchema, 'csv', largeDataset, { streaming: true });\n      const ndjsonResult = await convert(\n        UserArraySchema,\n        { from: 'csv', to: 'ndjson' },\n        csvResult,\n        { streaming: true }\n      );\n      const finalCsvResult = await convert(\n        UserArraySchema,\n        { from: 'ndjson', to: 'csv' },\n        ndjsonResult,\n        { streaming: true }\n      );\n\n      const endTime = Date.now();\n      const duration = endTime - startTime;\n\n      // Verify performance\n      expect(duration).toBeLessThan(15_000); // Should complete within 15 seconds\n      expect(finalCsvResult).toBeDefined();\n    });\n  });\n\n  describe('Adapter Interaction Workflows', () => {\n    it('should handle custom adapter registration and usage', async () => {\n      // Register a custom adapter\n      const customAdapter = {\n        async parse(input, opts = {}) {\n          // Custom parsing logic\n          const lines = input.split('\\n');\n          const data = lines.map(line => {\n            const [id, name, email] = line.split('|');\n            return { id, name, email, age: 30, active: true, metadata: { tags: [], score: 50 } };\n          });\n          return { data: { users: data, total: data.length, page: 1, limit: 10 }, metadata: {} };\n        },\n        async format(data, opts = {}) {\n          // Custom formatting logic\n          const lines = data.users.map(user => `${user.id}|${user.name}|${user.email}`);\n          return { data: lines.join('\\n'), metadata: {} };\n        },\n        supportsStreaming: true,\n        isAI: false,\n        version: '1.0.0',\n      };\n\n      registerAdapter('custom', customAdapter);\n\n      // Test the custom adapter\n      const input = 'user-1|John Doe|john@example.com\\nuser-2|Jane Smith|jane@example.com';\n      const result = await parseFrom(UserArraySchema, 'custom', input);\n\n      expect(result.users).toHaveLength(2);\n      expect(result.users[0].name).toBe('John Doe');\n      expect(result.users[1].name).toBe('Jane Smith');\n\n      // Test formatting\n      const formatted = await formatTo(UserArraySchema, 'custom', result);\n      expect(formatted).toContain('user-1|John Doe|john@example.com');\n      expect(formatted).toContain('user-2|Jane Smith|jane@example.com');\n    });\n\n    it('should handle adapter chaining and composition', async () => {\n      const userData = {\n        users: [\n          {\n            id: 'user-1',\n            name: 'John Doe',\n            email: 'john@example.com',\n            age: 30,\n            active: true,\n            metadata: {\n              tags: ['premium'],\n              score: 85,\n            },\n          },\n        ],\n        total: 1,\n        page: 1,\n        limit: 10,\n      };\n\n      // Chain multiple conversions\n      const step1 = await formatTo(UserArraySchema, 'json', userData, { deterministic: true });\n      const step2 = await convert(UserArraySchema, { from: 'json', to: 'csv' }, step1);\n      const step3 = await convert(UserArraySchema, { from: 'csv', to: 'ndjson' }, step2);\n      const step4 = await convert(UserArraySchema, { from: 'ndjson', to: 'json' }, step3);\n      const step5 = await convert(UserArraySchema, { from: 'json', to: 'csv' }, step4);\n\n      // Verify each step produces valid output\n      expect(step1).toBeDefined();\n      expect(step2).toBeDefined();\n      expect(step3).toBeDefined();\n      expect(step4).toBeDefined();\n      expect(step5).toBeDefined();\n\n      // Verify final result maintains data integrity\n      const finalParsed = await parseFrom(UserArraySchema, 'csv', step5);\n      expect(finalParsed.users).toHaveLength(1);\n      expect(finalParsed.users[0].name).toBe('John Doe');\n    });\n\n    it('should handle mixed adapter types in workflow', async () => {\n      const userData = {\n        users: [\n          {\n            id: 'user-1',\n            name: 'John Doe',\n            email: 'john@example.com',\n            age: 30,\n            active: true,\n            metadata: {\n              tags: ['premium'],\n              score: 85,\n            },\n          },\n        ],\n        total: 1,\n        page: 1,\n        limit: 10,\n      };\n\n      // Mix streaming and non-streaming adapters\n      const jsonResult = await formatTo(UserArraySchema, 'json', userData, { deterministic: true });\n      const csvResult = await convert(UserArraySchema, { from: 'json', to: 'csv' }, jsonResult, {\n        streaming: true,\n      });\n      const ndjsonResult = await convert(\n        UserArraySchema,\n        { from: 'csv', to: 'ndjson' },\n        csvResult,\n        { streaming: true }\n      );\n      const finalJsonResult = await convert(\n        UserArraySchema,\n        { from: 'ndjson', to: 'json' },\n        ndjsonResult\n      );\n\n      // Verify data integrity\n      const finalParsed = JSON.parse(finalJsonResult);\n      expect(finalParsed.users).toHaveLength(1);\n      expect(finalParsed.users[0].name).toBe('John Doe');\n    });\n  });\n\n  describe('Error Recovery Workflows', () => {\n    it('should handle partial failures in complex workflows', async () => {\n      const userData = {\n        users: [\n          {\n            id: 'user-1',\n            name: 'John Doe',\n            email: 'john@example.com',\n            age: 30,\n            active: true,\n            metadata: {\n              tags: ['premium'],\n              score: 85,\n            },\n          },\n        ],\n        total: 1,\n        page: 1,\n        limit: 10,\n      };\n\n      // Start with valid data\n      const jsonResult = await formatTo(UserArraySchema, 'json', userData, { deterministic: true });\n      const csvResult = await convert(UserArraySchema, { from: 'json', to: 'csv' }, jsonResult);\n\n      // Introduce an error by trying to parse malformed data\n      const malformedCsv = csvResult + '\\ninvalid,data,row';\n\n      try {\n        await parseFrom(UserArraySchema, 'csv', malformedCsv);\n        expect.fail('Should have thrown an error');\n      } catch (error) {\n        expect(error).toBeDefined();\n      }\n\n      // Verify that we can still process valid data after the error\n      const recoveredResult = await parseFrom(UserArraySchema, 'csv', csvResult);\n      expect(recoveredResult.users).toHaveLength(1);\n      expect(recoveredResult.users[0].name).toBe('John Doe');\n    });\n\n    it('should handle schema validation errors in workflows', async () => {\n      const invalidUserData = {\n        users: [\n          {\n            id: 'user-1',\n            name: 'John Doe',\n            email: 'invalid-email', // Invalid email format\n            age: 30,\n            active: true,\n            metadata: {\n              tags: ['premium'],\n              score: 85,\n            },\n          },\n        ],\n        total: 1,\n        page: 1,\n        limit: 10,\n      };\n\n      // Should fail at format stage due to invalid email\n      try {\n        await formatTo(UserArraySchema, 'json', invalidUserData);\n        expect.fail('Should have thrown an error');\n      } catch (error) {\n        expect(error).toBeDefined();\n        expect(error.message).toContain('email');\n      }\n\n      // Verify that valid data still works\n      const validUserData = {\n        ...invalidUserData,\n        users: [\n          {\n            ...invalidUserData.users[0],\n            email: 'john@example.com', // Valid email\n          },\n        ],\n      };\n\n      const result = await formatTo(UserArraySchema, 'json', validUserData);\n      expect(result).toBeDefined();\n    });\n  });\n\n  describe('Performance Integration Tests', () => {\n    it('should handle concurrent complex workflows', async () => {\n      const userData = {\n        users: Array.from({ length: 100 }, (_, i) => ({\n          id: `user-${i + 1}`,\n          name: `User ${i + 1}`,\n          email: `user${i + 1}@example.com`,\n          age: 20 + (i % 50),\n          active: i % 2 === 0,\n          metadata: {\n            tags: [`tag-${i % 10}`],\n            score: 50 + (i % 50),\n          },\n        })),\n        total: 100,\n        page: 1,\n        limit: 100,\n      };\n\n      // Create multiple concurrent workflows\n      const workflows = Array.from({ length: 10 }, (_, i) =>\n        convert(UserArraySchema, { from: 'json', to: 'csv' }, JSON.stringify(userData))\n      );\n\n      const startTime = Date.now();\n      const results = await Promise.all(workflows);\n      const endTime = Date.now();\n      const duration = endTime - startTime;\n\n      // Verify all workflows completed successfully\n      expect(results).toHaveLength(10);\n      for (const result of results) {\n        expect(result).toBeDefined();\n        expect(result).toContain('id,name,email,age,active');\n      }\n\n      // Verify performance\n      expect(duration).toBeLessThan(5000); // Should complete within 5 seconds\n    });\n\n    it('should handle memory-efficient large data processing', async () => {\n      const largeDataset = {\n        users: Array.from({ length: 10_000 }, (_, i) => ({\n          id: `user-${i + 1}`,\n          name: `User ${i + 1}`,\n          email: `user${i + 1}@example.com`,\n          age: 20 + (i % 50),\n          active: i % 2 === 0,\n          metadata: {\n            tags: [`tag-${i % 10}`],\n            score: 50 + (i % 50),\n          },\n        })),\n        total: 10_000,\n        page: 1,\n        limit: 10_000,\n      };\n\n      const startTime = Date.now();\n      const memoryBefore = process.memoryUsage();\n\n      // Process large dataset with streaming\n      const csvResult = await formatTo(UserArraySchema, 'csv', largeDataset, { streaming: true });\n      const ndjsonResult = await convert(\n        UserArraySchema,\n        { from: 'csv', to: 'ndjson' },\n        csvResult,\n        { streaming: true }\n      );\n      const finalCsvResult = await convert(\n        UserArraySchema,\n        { from: 'ndjson', to: 'csv' },\n        ndjsonResult,\n        { streaming: true }\n      );\n\n      const endTime = Date.now();\n      const memoryAfter = process.memoryUsage();\n      const duration = endTime - startTime;\n\n      // Verify results\n      expect(finalCsvResult).toBeDefined();\n      expect(duration).toBeLessThan(30_000); // Should complete within 30 seconds\n\n      // Verify memory usage is reasonable (not more than 2x initial)\n      const memoryIncrease = memoryAfter.heapUsed - memoryBefore.heapUsed;\n      expect(memoryIncrease).toBeLessThan(memoryBefore.heapUsed * 2);\n    });\n  });\n\n  describe('Real-World Scenario Tests', () => {\n    it('should handle data export workflow', async () => {\n      // Simulate exporting user data from database to multiple formats\n      const userData = {\n        users: [\n          {\n            id: 'user-1',\n            name: 'John Doe',\n            email: 'john@example.com',\n            age: 30,\n            active: true,\n            metadata: {\n              tags: ['premium', 'verified'],\n              score: 85,\n              lastLogin: '2024-01-15T10:30:00Z',\n            },\n          },\n          {\n            id: 'user-2',\n            name: 'Jane Smith',\n            email: 'jane@example.com',\n            age: 25,\n            active: false,\n            metadata: {\n              tags: ['basic'],\n              score: 60,\n            },\n          },\n        ],\n        total: 2,\n        page: 1,\n        limit: 10,\n      };\n\n      // Export to JSON\n      const jsonExport = await formatTo(UserArraySchema, 'json', userData, { deterministic: true });\n\n      // Export to CSV\n      const csvExport = await formatTo(UserArraySchema, 'csv', userData);\n\n      // Export to NDJSON\n      const ndjsonExport = await formatTo(UserArraySchema, 'ndjson', userData);\n\n      // Verify all exports are valid\n      expect(jsonExport).toBeDefined();\n      expect(csvExport).toBeDefined();\n      expect(ndjsonExport).toBeDefined();\n\n      // Verify JSON export can be parsed back\n      const jsonParsed = JSON.parse(jsonExport);\n      expect(jsonParsed.users).toHaveLength(2);\n\n      // Verify CSV export can be parsed back\n      const csvParsed = await parseFrom(UserArraySchema, 'csv', csvExport);\n      expect(csvParsed.users).toHaveLength(2);\n\n      // Verify NDJSON export can be parsed back\n      const ndjsonParsed = await parseFrom(UserArraySchema, 'ndjson', ndjsonExport);\n      expect(ndjsonParsed.users).toHaveLength(2);\n    });\n\n    it('should handle data import workflow', async () => {\n      // Simulate importing user data from external sources\n      const csvImport = `id,name,email,age,active\nuser-1,John Doe,john@example.com,30,true\nuser-2,Jane Smith,jane@example.com,25,false`;\n\n      const ndjsonImport = `{\"id\": \"user-3\", \"name\": \"Bob Johnson\", \"email\": \"bob@example.com\", \"age\": 35, \"active\": true}\n{\"id\": \"user-4\", \"name\": \"Alice Brown\", \"email\": \"alice@example.com\", \"age\": 28, \"active\": false}`;\n\n      // Import from CSV\n      const csvData = await parseFrom(UserArraySchema, 'csv', csvImport);\n\n      // Import from NDJSON\n      const ndjsonData = await parseFrom(UserArraySchema, 'ndjson', ndjsonImport);\n\n      // Verify imports\n      expect(csvData.users).toHaveLength(2);\n      expect(ndjsonData.users).toHaveLength(2);\n\n      // Merge and export\n      const mergedData = {\n        users: [...csvData.users, ...ndjsonData.users],\n        total: 4,\n        page: 1,\n        limit: 10,\n      };\n\n      const finalExport = await formatTo(UserArraySchema, 'json', mergedData, {\n        deterministic: true,\n      });\n      const finalParsed = JSON.parse(finalExport);\n\n      expect(finalParsed.users).toHaveLength(4);\n      expect(finalParsed.total).toBe(4);\n    });\n\n    it('should handle data transformation workflow', async () => {\n      // Simulate transforming data between different systems\n      const sourceData = {\n        users: [\n          {\n            id: 'user-1',\n            name: 'John Doe',\n            email: 'john@example.com',\n            age: 30,\n            active: true,\n            metadata: {\n              tags: ['premium'],\n              score: 85,\n            },\n          },\n        ],\n        total: 1,\n        page: 1,\n        limit: 10,\n      };\n\n      // Transform: JSON -> CSV -> NDJSON -> JSON\n      const step1 = await formatTo(UserArraySchema, 'json', sourceData, { deterministic: true });\n      const step2 = await convert(UserArraySchema, { from: 'json', to: 'csv' }, step1);\n      const step3 = await convert(UserArraySchema, { from: 'csv', to: 'ndjson' }, step2);\n      const step4 = await convert(UserArraySchema, { from: 'ndjson', to: 'json' }, step3);\n\n      // Verify transformation maintains data integrity\n      const originalParsed = JSON.parse(step1);\n      const finalParsed = JSON.parse(step4);\n\n      expect(originalParsed.users).toEqual(finalParsed.users);\n      expect(originalParsed.total).toBe(finalParsed.total);\n      expect(originalParsed.page).toBe(finalParsed.page);\n      expect(originalParsed.limit).toBe(finalParsed.limit);\n    });\n  });\n});\n"
        }
    ]
}