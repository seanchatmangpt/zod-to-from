{
    "sourceFile": "test/features/streaming.test.mjs",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 1,
            "patches": [
                {
                    "date": 1758645130119,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1758645941402,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -4,10 +4,9 @@\n  */\n \n import { beforeEach, describe, expect, it } from 'vitest';\n import { z } from 'zod';\n-import { convert, formatTo, parseFrom } from '../../src/core/main.mjs';\n-import { adapterSupports } from '../../src/core/registry.mjs';\n+import { adapterSupports, convert, formatTo, parseFrom } from '../setup.mjs';\n \n describe('Streaming Features', () => {\n   const StreamingSchema = z.object({\n     items: z.array(\n"
                }
            ],
            "date": 1758645130119,
            "name": "Commit-0",
            "content": "/**\n * Streaming Feature Tests\n * @fileoverview Tests for streaming functionality across adapters\n */\n\nimport { beforeEach, describe, expect, it } from 'vitest';\nimport { z } from 'zod';\nimport { convert, formatTo, parseFrom } from '../../src/core/main.mjs';\nimport { adapterSupports } from '../../src/core/registry.mjs';\n\ndescribe('Streaming Features', () => {\n  const StreamingSchema = z.object({\n    items: z.array(\n      z.object({\n        id: z.number(),\n        name: z.string(),\n        value: z.number(),\n      })\n    ),\n  });\n\n  beforeEach(() => {\n    // Ensure clean state for each test\n  });\n\n  describe('Streaming Support Detection', () => {\n    it('should correctly identify streaming-capable adapters', () => {\n      expect(adapterSupports('csv', 'streaming')).toBe(true);\n      expect(adapterSupports('ndjson', 'streaming')).toBe(true);\n      expect(adapterSupports('json', 'streaming')).toBe(false);\n    });\n\n    it('should return false for non-existent adapters', () => {\n      expect(adapterSupports('non-existent', 'streaming')).toBe(false);\n    });\n  });\n\n  describe('CSV Streaming', () => {\n    it('should handle streaming CSV parsing', async () => {\n      const csvInput = `id,name,value\n1,Item 1,10\n2,Item 2,20\n3,Item 3,30`;\n\n      const result = await parseFrom(StreamingSchema, 'csv', csvInput, { streaming: true });\n\n      expect(result.items).toHaveLength(3);\n      expect(result.items[0]).toEqual({ id: 1, name: 'Item 1', value: 10 });\n      expect(result.items[1]).toEqual({ id: 2, name: 'Item 2', value: 20 });\n      expect(result.items[2]).toEqual({ id: 3, name: 'Item 3', value: 30 });\n    });\n\n    it('should handle streaming CSV formatting', async () => {\n      const data = {\n        items: [\n          { id: 1, name: 'Item 1', value: 10 },\n          { id: 2, name: 'Item 2', value: 20 },\n          { id: 3, name: 'Item 3', value: 30 },\n        ],\n      };\n\n      const result = await formatTo(StreamingSchema, 'csv', data, { streaming: true });\n\n      expect(result).toContain('id,name,value');\n      expect(result).toContain('1,Item 1,10');\n      expect(result).toContain('2,Item 2,20');\n      expect(result).toContain('3,Item 3,30');\n    });\n\n    it('should handle large CSV files efficiently', async () => {\n      // Generate large CSV\n      const headers = 'id,name,value\\n';\n      const rows = Array.from(\n        { length: 1000 },\n        (_, i) => `${i + 1},Item ${i + 1},${(i + 1) * 10}`\n      ).join('\\n');\n      const largeCSV = headers + rows;\n\n      const startTime = Date.now();\n      const result = await parseFrom(StreamingSchema, 'csv', largeCSV, { streaming: true });\n      const endTime = Date.now();\n\n      expect(result.items).toHaveLength(1000);\n      expect(endTime - startTime).toBeLessThan(1000); // Should complete within 1 second\n    });\n\n    it('should handle streaming CSV conversion', async () => {\n      const csvInput = `id,name,value\n1,Item 1,10\n2,Item 2,20`;\n\n      const result = await convert(StreamingSchema, { from: 'csv', to: 'csv' }, csvInput, {\n        streaming: true,\n      });\n\n      expect(result).toContain('id,name,value');\n      expect(result).toContain('1,Item 1,10');\n      expect(result).toContain('2,Item 2,20');\n    });\n  });\n\n  describe('NDJSON Streaming', () => {\n    it('should handle streaming NDJSON parsing', async () => {\n      const ndjsonInput = `{\"id\": 1, \"name\": \"Item 1\", \"value\": 10}\n{\"id\": 2, \"name\": \"Item 2\", \"value\": 20}\n{\"id\": 3, \"name\": \"Item 3\", \"value\": 30}`;\n\n      const result = await parseFrom(StreamingSchema, 'ndjson', ndjsonInput, { streaming: true });\n\n      expect(result.items).toHaveLength(3);\n      expect(result.items[0]).toEqual({ id: 1, name: 'Item 1', value: 10 });\n      expect(result.items[1]).toEqual({ id: 2, name: 'Item 2', value: 20 });\n      expect(result.items[2]).toEqual({ id: 3, name: 'Item 3', value: 30 });\n    });\n\n    it('should handle streaming NDJSON formatting', async () => {\n      const data = {\n        items: [\n          { id: 1, name: 'Item 1', value: 10 },\n          { id: 2, name: 'Item 2', value: 20 },\n          { id: 3, name: 'Item 3', value: 30 },\n        ],\n      };\n\n      const result = await formatTo(StreamingSchema, 'ndjson', data, { streaming: true });\n\n      const lines = result.trim().split('\\n');\n      expect(lines).toHaveLength(3);\n      expect(JSON.parse(lines[0])).toEqual({ id: 1, name: 'Item 1', value: 10 });\n      expect(JSON.parse(lines[1])).toEqual({ id: 2, name: 'Item 2', value: 20 });\n      expect(JSON.parse(lines[2])).toEqual({ id: 3, name: 'Item 3', value: 30 });\n    });\n\n    it('should handle large NDJSON files efficiently', async () => {\n      // Generate large NDJSON\n      const lines = Array.from({ length: 1000 }, (_, i) =>\n        JSON.stringify({ id: i + 1, name: `Item ${i + 1}`, value: (i + 1) * 10 })\n      ).join('\\n');\n\n      const startTime = Date.now();\n      const result = await parseFrom(StreamingSchema, 'ndjson', lines, { streaming: true });\n      const endTime = Date.now();\n\n      expect(result.items).toHaveLength(1000);\n      expect(endTime - startTime).toBeLessThan(1000); // Should complete within 1 second\n    });\n\n    it('should handle streaming NDJSON conversion', async () => {\n      const ndjsonInput = `{\"id\": 1, \"name\": \"Item 1\", \"value\": 10}\n{\"id\": 2, \"name\": \"Item 2\", \"value\": 20}`;\n\n      const result = await convert(StreamingSchema, { from: 'ndjson', to: 'ndjson' }, ndjsonInput, {\n        streaming: true,\n      });\n\n      const lines = result.trim().split('\\n');\n      expect(lines).toHaveLength(2);\n      expect(JSON.parse(lines[0])).toEqual({ id: 1, name: 'Item 1', value: 10 });\n      expect(JSON.parse(lines[1])).toEqual({ id: 2, name: 'Item 2', value: 20 });\n    });\n  });\n\n  describe('Cross-Format Streaming Conversion', () => {\n    it('should convert CSV to NDJSON with streaming', async () => {\n      const csvInput = `id,name,value\n1,Item 1,10\n2,Item 2,20`;\n\n      const result = await convert(StreamingSchema, { from: 'csv', to: 'ndjson' }, csvInput, {\n        streaming: true,\n      });\n\n      const lines = result.trim().split('\\n');\n      expect(lines).toHaveLength(2);\n      expect(JSON.parse(lines[0])).toEqual({ id: 1, name: 'Item 1', value: 10 });\n      expect(JSON.parse(lines[1])).toEqual({ id: 2, name: 'Item 2', value: 20 });\n    });\n\n    it('should convert NDJSON to CSV with streaming', async () => {\n      const ndjsonInput = `{\"id\": 1, \"name\": \"Item 1\", \"value\": 10}\n{\"id\": 2, \"name\": \"Item 2\", \"value\": 20}`;\n\n      const result = await convert(StreamingSchema, { from: 'ndjson', to: 'csv' }, ndjsonInput, {\n        streaming: true,\n      });\n\n      expect(result).toContain('id,name,value');\n      expect(result).toContain('1,Item 1,10');\n      expect(result).toContain('2,Item 2,20');\n    });\n\n    it('should convert streaming formats to JSON', async () => {\n      const csvInput = `id,name,value\n1,Item 1,10\n2,Item 2,20`;\n\n      const result = await convert(StreamingSchema, { from: 'csv', to: 'json' }, csvInput, {\n        streaming: true,\n      });\n\n      const parsedResult = JSON.parse(result);\n      expect(parsedResult.items).toHaveLength(2);\n      expect(parsedResult.items[0]).toEqual({ id: 1, name: 'Item 1', value: 10 });\n      expect(parsedResult.items[1]).toEqual({ id: 2, name: 'Item 2', value: 20 });\n    });\n\n    it('should convert JSON to streaming formats', async () => {\n      const jsonInput = JSON.stringify({\n        items: [\n          { id: 1, name: 'Item 1', value: 10 },\n          { id: 2, name: 'Item 2', value: 20 },\n        ],\n      });\n\n      const result = await convert(StreamingSchema, { from: 'json', to: 'csv' }, jsonInput, {\n        streaming: true,\n      });\n\n      expect(result).toContain('id,name,value');\n      expect(result).toContain('1,Item 1,10');\n      expect(result).toContain('2,Item 2,20');\n    });\n  });\n\n  describe('Streaming Error Handling', () => {\n    it('should throw error for streaming on non-streaming adapters', async () => {\n      const input = '{\"name\": \"Test\", \"age\": 30, \"active\": true}';\n\n      await expect(parseFrom(StreamingSchema, 'json', input, { streaming: true })).rejects.toThrow(\n        \"Adapter 'json' does not support streaming\"\n      );\n    });\n\n    it('should handle streaming errors gracefully', async () => {\n      const malformedCSV = `id,name,value\n1,Item 1,10\n2,Item 2`; // Missing value column\n\n      await expect(\n        parseFrom(StreamingSchema, 'csv', malformedCSV, { streaming: true })\n      ).rejects.toThrow();\n    });\n\n    it('should handle large streaming operations without memory issues', async () => {\n      // Generate very large CSV\n      const headers = 'id,name,value\\n';\n      const rows = Array.from(\n        { length: 10_000 },\n        (_, i) => `${i + 1},Item ${i + 1},${(i + 1) * 10}`\n      ).join('\\n');\n      const veryLargeCSV = headers + rows;\n\n      const startTime = Date.now();\n      const result = await parseFrom(StreamingSchema, 'csv', veryLargeCSV, { streaming: true });\n      const endTime = Date.now();\n\n      expect(result.items).toHaveLength(10_000);\n      expect(endTime - startTime).toBeLessThan(5000); // Should complete within 5 seconds\n    });\n  });\n\n  describe('Streaming Metadata', () => {\n    it('should include streaming metadata in parse results', async () => {\n      const csvInput = `id,name,value\n1,Item 1,10\n2,Item 2,20`;\n\n      const result = await parseFrom(StreamingSchema, 'csv', csvInput, {\n        streaming: true,\n        includeProvenance: true,\n      });\n\n      expect(result).toHaveProperty('data');\n      expect(result).toHaveProperty('provenance');\n      expect(result.provenance).toHaveProperty('adapter', 'csv');\n      expect(result.provenance).toHaveProperty('sourceFormat', 'csv');\n    });\n\n    it('should include streaming metadata in format results', async () => {\n      const data = {\n        items: [{ id: 1, name: 'Item 1', value: 10 }],\n      };\n\n      const result = await formatTo(StreamingSchema, 'csv', data, {\n        streaming: true,\n        includeProvenance: true,\n      });\n\n      expect(result).toHaveProperty('data');\n      expect(result).toHaveProperty('provenance');\n      expect(result.provenance).toHaveProperty('adapter', 'csv');\n      expect(result.provenance).toHaveProperty('targetFormat', 'csv');\n    });\n  });\n\n  describe('Streaming Performance', () => {\n    it('should demonstrate streaming performance benefits', async () => {\n      // Generate large dataset\n      const headers = 'id,name,value\\n';\n      const rows = Array.from(\n        { length: 5000 },\n        (_, i) => `${i + 1},Item ${i + 1},${(i + 1) * 10}`\n      ).join('\\n');\n      const largeCSV = headers + rows;\n\n      // Test with streaming\n      const streamingStart = Date.now();\n      const streamingResult = await parseFrom(StreamingSchema, 'csv', largeCSV, {\n        streaming: true,\n      });\n      const streamingEnd = Date.now();\n      const streamingTime = streamingEnd - streamingStart;\n\n      // Test without streaming (should be same for CSV, but demonstrates the API)\n      const nonStreamingStart = Date.now();\n      const nonStreamingResult = await parseFrom(StreamingSchema, 'csv', largeCSV, {\n        streaming: false,\n      });\n      const nonStreamingEnd = Date.now();\n      const nonStreamingTime = nonStreamingEnd - nonStreamingStart;\n\n      expect(streamingResult.items).toHaveLength(5000);\n      expect(nonStreamingResult.items).toHaveLength(5000);\n      expect(streamingResult.items).toEqual(nonStreamingResult.items);\n\n      // Both should complete reasonably quickly\n      expect(streamingTime).toBeLessThan(2000);\n      expect(nonStreamingTime).toBeLessThan(2000);\n    });\n  });\n});\n"
        }
    ]
}